{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#SVM & Naive bayes"
      ],
      "metadata": {
        "id": "_YyNaFPYSuFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n",
        "-> **Support Vector Machine (SVM)** is a supervised learning algorithm used for **classification** and **regression**.\n",
        "It finds the **best boundary (hyperplane)** that separates different classes and **maximizes the margin** between them.\n",
        "The closest data points to this boundary are called **support vectors**.\n",
        "SVM works well for **clear and high-dimensional** data but is **slow for large datasets**.\n"
      ],
      "metadata": {
        "id": "6H6BiRt3S0cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the difference between Hard Margin and Soft Margin SVM?\n",
        "-> 🔍 Difference between **Hard Margin** and **Soft Margin** SVM:\n",
        "\n",
        "| Feature                 | **Hard Margin SVM**                  | **Soft Margin SVM**                          |\n",
        "| ----------------------- | ------------------------------------ | -------------------------------------------- |\n",
        "| **Tolerance to Errors** | No tolerance (no misclassification)  | Allows some misclassification                |\n",
        "| **Data Requirement**    | Data must be **perfectly separable** | Works with **overlapping/noisy** data        |\n",
        "| **Flexibility**         | Very strict (not flexible)           | More flexible, better real-world performance |\n",
        "| **Overfitting Risk**    | High, if data is not clean           | Lower, handles noise better                  |\n",
        "\n",
        "### 🧠 In Short:\n",
        "\n",
        "* **Hard Margin** = Strict, perfect separation, no errors.\n",
        "* **Soft Margin** = Flexible, allows some errors for better generalization.\n"
      ],
      "metadata": {
        "id": "HPjUtqz_TIxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM?\n",
        "->  🧠 Mathematical Intuition Behind SVM (in Simple Words):\n",
        "\n",
        "SVM tries to **find the best hyperplane** (a line in 2D, a plane in 3D) that **separates** the classes **with the largest margin**.\n",
        "\n",
        "\n",
        "\n",
        "### 📐 Goal:\n",
        "\n",
        "Maximize the **margin** = distance between the hyperplane and the nearest data points (support vectors).\n",
        "\n",
        "\n",
        "\n",
        "### ✏️ Mathematically:\n",
        "\n",
        "We want to find:\n",
        "\n",
        "```\n",
        "w · x + b = 0      → Equation of hyperplane\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* `w` = weight vector (direction of the plane)\n",
        "* `x` = input features\n",
        "* `b` = bias (offset)\n",
        "\n",
        "\n",
        "\n",
        "### 📌 Optimization Problem:\n",
        "\n",
        "We minimize:\n",
        "\n",
        "```\n",
        "||w||² / 2   → (keep weights small = wider margin)\n",
        "```\n",
        "\n",
        "Subject to:\n",
        "\n",
        "```\n",
        "yᵢ (w · xᵢ + b) ≥ 1   → For all data points (xᵢ, yᵢ)\n",
        "```\n",
        "\n",
        "> If soft margin: allow some slack variables (ξ) to handle misclassifications.\n",
        "\n",
        "\n",
        "\n",
        "### ✅ Intuition:\n",
        "\n",
        "* A **larger margin** = better generalization.\n",
        "* SVM uses **convex optimization** → always finds a global minimum.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2aACeR4MTWuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM?\n",
        "->  📌 Role of **Lagrange Multipliers** in SVM (Simple Explanation):\n",
        "\n",
        "When solving the SVM optimization problem, we want to:\n",
        "\n",
        "> **Maximize the margin** while keeping **all data points correctly classified**.\n",
        "\n",
        "This is a **constrained optimization** problem — so we use **Lagrange Multipliers** to solve it.\n",
        "\n",
        "\n",
        "### 🎯 Why Use Lagrange Multipliers?\n",
        "\n",
        "* They help **combine** the main objective (maximize margin) with the **constraints** (correct classification).\n",
        "* They let us **convert the problem** into a form that’s easier to solve using calculus.\n",
        "\n",
        "\n",
        "### 🧠 Intuition:\n",
        "\n",
        "We want to:\n",
        "\n",
        "**Minimize:**\n",
        "\n",
        "$$\n",
        "\\frac{1}{2} ||w||^2\n",
        "$$\n",
        "\n",
        "**Subject to:**\n",
        "\n",
        "$$\n",
        "yᵢ (w · xᵢ + b) ≥ 1\n",
        "$$\n",
        "\n",
        "Using **Lagrange Multipliers (αᵢ)**, we create a new function (called the **Lagrangian**) that mixes both parts.\n",
        "\n",
        "\n",
        "### ✅ Final Goal:\n",
        "\n",
        "Use these αᵢ values to:\n",
        "\n",
        "* Find the optimal **weights (w)** and **bias (b)**\n",
        "* Identify **support vectors** (they are the points where αᵢ > 0)\n",
        "\n"
      ],
      "metadata": {
        "id": "TGKfqkp2Tujt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What are Support Vectors in SVM?\n",
        "-> **Support Vectors** are the **data points that lie closest to the decision boundary (hyperplane)**.\n",
        "\n",
        "\n",
        "### 🧠 Why are they important?\n",
        "\n",
        "* They are the **most critical points** in the dataset.\n",
        "* They **define the position and orientation** of the hyperplane.\n",
        "* If you remove a support vector, the hyperplane may **change**.\n",
        "\n",
        "\n",
        "### ✅ In Simple Words:\n",
        "\n",
        "Support vectors are the **\"borderline\" points** that SVM uses to **draw the best separating line**.\n",
        "\n",
        "They **support** or **hold up** the margin — that’s why they are called **support vectors**.\n",
        "\n"
      ],
      "metadata": {
        "id": "DlwME3rRUDhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)?\n",
        "-> **Support Vector Classifier (SVC)** is the **classification version** of the Support Vector Machine (SVM).\n",
        "\n",
        "\n",
        "### 🧠 Simple Explanation:\n",
        "\n",
        "SVC is a machine learning model that:\n",
        "\n",
        "* **Draws the best boundary (hyperplane)** to separate different classes.\n",
        "* Uses **support vectors** to create the widest possible margin between classes.\n",
        "* Can work in **linear** or **non-linear** cases (using kernel trick).\n",
        "\n",
        "\n",
        "### 🧾 In Short:\n",
        "\n",
        "> **SVC = SVM used for classification tasks.**\n",
        "\n",
        "It predicts which class a data point belongs to by using the hyperplane defined by support vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "QEdbouJuUWyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)?\n",
        "->\n",
        "**Support Vector Regressor (SVR)** is the **regression version** of Support Vector Machine (SVM).\n",
        "\n",
        "\n",
        "### 🧠 Simple Explanation:\n",
        "\n",
        "* SVR tries to **fit a line (or curve)** that **predicts continuous values**.\n",
        "* It aims to keep the **predicted values within a margin (epsilon)** from the actual values.\n",
        "* Points **outside** this margin are treated as **errors**, and SVR tries to minimize them.\n",
        "\n",
        "\n",
        "### ✏️ Key Idea:\n",
        "\n",
        "Instead of drawing a boundary between classes (like SVC), SVR tries to:\n",
        "\n",
        "> **Keep most data points inside a \"tube\" around the prediction line.**\n",
        "\n",
        "### ✅ In Short:\n",
        "\n",
        "> **SVR = SVM for regression tasks**, where we predict **numbers** instead of **classes**.\n",
        "\n"
      ],
      "metadata": {
        "id": "YFbxKpLXUzm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the Kernel Trick in SVM?\n",
        "->The **Kernel Trick** lets SVM **solve non-linear problems** by **transforming data into a higher dimension**, **without actually doing the math**!\n",
        "\n",
        "\n",
        "### 📌 In Simple Words:\n",
        "\n",
        "* Some data can't be separated by a straight line (non-linear).\n",
        "* The kernel trick **converts data into a higher-dimensional space** where it **can be separated** by a straight line.\n",
        "* SVM does this **efficiently** using a **kernel function**, without needing to compute the actual transformation.\n",
        "\n",
        "### 🎯 Example:\n",
        "\n",
        "* Imagine a circle of red dots surrounded by blue dots — no straight line can separate them.\n",
        "* Kernel trick **lifts** the circle into 3D, where a **plane** can separate them.\n",
        "\n",
        "\n",
        "### 🔧 Common Kernels:\n",
        "\n",
        "* **Linear**: For straight-line separation.\n",
        "* **Polynomial**: For curved boundaries.\n",
        "* **RBF (Gaussian)**: For very flexible, complex shapes.\n"
      ],
      "metadata": {
        "id": "uMR4h10uVGwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel?\n",
        "->\n",
        "\n",
        "| Feature                 | **Linear Kernel**            | **Polynomial Kernel**                  | **RBF (Radial Basis Function) Kernel**  |\n",
        "| ----------------------- | ---------------------------- | -------------------------------------- | --------------------------------------- |\n",
        "| **Use Case**            | Linearly separable data      | Non-linear data with curved boundaries | Complex, highly non-linear data         |\n",
        "| **Equation**            | $K(x, x') = x \\cdot x'$      | $K(x, x') = (x \\cdot x' + c)^d$        | $K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$ |\n",
        "| **Parameters**          | None                         | Degree `d`, constant `c`               | Gamma (γ)                               |\n",
        "| **Speed**               | Fastest                      | Slower than linear                     | Slower, but very powerful               |\n",
        "| **Flexibility**         | Low (only linear boundaries) | Medium (depends on degree)             | High (can model complex shapes)         |\n",
        "| **Risk of Overfitting** | Low                          | Medium to high (if degree too high)    | High (if gamma is too large)            |\n",
        "\n",
        "\n",
        "### 🧠 In Simple Words:\n",
        "\n",
        "* **Linear**: Best when data is **linearly separable**.\n",
        "* **Polynomial**: Good for **moderate complexity**.\n",
        "* **RBF**: Best for **complex patterns**, works in most real-world cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "bvCTbPnYVhnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is the effect of the C parameter in SVM?\n",
        "->\n",
        "\n",
        "### 🧠 Simple Explanation:\n",
        "\n",
        "* **C** controls the **trade-off** between:\n",
        "\n",
        "  * **Maximizing the margin** (simpler model, more general)\n",
        "  * **Minimizing classification errors** (better fit to training data)\n",
        "\n",
        "\n",
        "### How it works:\n",
        "\n",
        "* **Large C**:\n",
        "\n",
        "  * The model tries hard to **avoid errors**.\n",
        "  * Smaller margin, **less tolerance for misclassification**.\n",
        "  * Can lead to **overfitting** (model fits training data very tightly).\n",
        "\n",
        "* **Small C**:\n",
        "\n",
        "  * The model allows **some errors**.\n",
        "  * Larger margin, **more tolerance for misclassification**.\n",
        "  * Can lead to **underfitting** (simpler, more general model).\n",
        "\n"
      ],
      "metadata": {
        "id": "qeU-B2QQV4Yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "->\n",
        "### 🧠 Simple Explanation:\n",
        "\n",
        "* **Gamma** controls how much influence a single training example has.\n",
        "\n",
        "\n",
        "### Details:\n",
        "\n",
        "* **High gamma**:\n",
        "\n",
        "  * Each point’s influence is **very close** to itself.\n",
        "  * Decision boundary becomes **very wiggly** (fits tightly to training data).\n",
        "  * Can cause **overfitting**.\n",
        "\n",
        "* **Low gamma**:\n",
        "\n",
        "  * Points have influence over a **larger area**.\n",
        "  * Decision boundary is **smoother**.\n",
        "  * Can cause **underfitting**.\n",
        "\n"
      ],
      "metadata": {
        "id": "4odiT7UdWVYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "->\n",
        "* It's a **simple probabilistic classifier** based on **Bayes’ Theorem**.\n",
        "* It predicts the class of data by calculating the **probability of each class given the features**.\n",
        "* It assumes that **all features are independent of each other**.\n",
        "\n",
        "\n",
        "### Why is it called **\"Naïve\"**?\n",
        "\n",
        "Because it **naively assumes that all features are independent**, even though in real life, features often depend on each other. This simplification makes calculations easier but is not always true.\n"
      ],
      "metadata": {
        "id": "l3TdcTndW6Yj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes’ Theorem?\n",
        "-> **Bayes’ Theorem** is a rule for updating the probability of a hypothesis (A) given new evidence (B). It states:\n",
        "\n",
        "$$\n",
        "P(A \\mid B) \\;=\\; \\frac{P(B \\mid A)\\,P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "* **P(A)** = prior probability of A (before seeing B)\n",
        "* **P(B)** = total probability of B\n",
        "* **P(B | A)** = likelihood of seeing B if A is true\n",
        "* **P(A | B)** = updated (posterior) probability of A after seeing B\n",
        "\n",
        "In simple terms, Bayes’ Theorem tells us how to revise our belief in A once we observe B.\n"
      ],
      "metadata": {
        "id": "xaXVbm9ZXb0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes?\n",
        "->\n",
        "\n",
        "| Feature              | **Gaussian Naïve Bayes**                           | **Multinomial Naïve Bayes**                             | **Bernoulli Naïve Bayes**                  |\n",
        "| -------------------- | -------------------------------------------------- | ------------------------------------------------------- | ------------------------------------------ |\n",
        "| **Data Type**        | Continuous (real numbers)                          | Discrete counts (e.g., word frequencies)                | Binary (0 or 1 values)                     |\n",
        "| **Example Use Case** | Predicting income, height, etc.                    | Text classification (spam detection, news topics)       | Binary feature text (word present or not)  |\n",
        "| **Assumes**          | Features follow **normal (Gaussian)** distribution | Features are **counts** from a multinomial distribution | Features are **binary** (yes/no, on/off)   |\n",
        "| **Formula Used**     | Uses probability density function (PDF)            | Uses count-based likelihoods                            | Uses binary probability (presence/absence) |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 In Short:\n",
        "\n",
        "* **Gaussian NB** → Use for **continuous** data\n",
        "* **Multinomial NB** → Use for **count-based** (e.g., word counts in text)\n",
        "* **Bernoulli NB** → Use for **binary** data (e.g., word present or not)\n",
        "\n"
      ],
      "metadata": {
        "id": "nPE5QA6Bj0C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Naïve Bayes over other variants?\n",
        "->\n",
        "Use **Gaussian Naïve Bayes** when your **features are continuous (real-valued numbers)** and **follow a normal (bell-shaped) distribution**.\n",
        "\n",
        "### 📌 Best Use Cases:\n",
        "\n",
        "* Medical data (e.g., blood pressure, cholesterol level)\n",
        "* Sensor readings\n",
        "* Financial data (e.g., income, age, salary)\n",
        "\n",
        "\n",
        "### 🚫 Don’t use it when:\n",
        "\n",
        "* Features are **counts** → use **Multinomial NB**\n",
        "* Features are **binary (0/1)** → use **Bernoulli NB**\n"
      ],
      "metadata": {
        "id": "eC9H7bugmAHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Naïve Bayes?\n",
        "->\n",
        "1. **Feature Independence**\n",
        "   👉 All features are **independent** of each other given the class label.\n",
        "   *(This is the \"naïve\" part — in real life, features often depend on each other.)*\n",
        "\n",
        "2. **Feature Contribution is Equal**\n",
        "   👉 Each feature contributes **equally and independently** to the final decision.\n",
        "\n",
        "3. **Data Distribution Depends on the Variant**\n",
        "\n",
        "   * **Gaussian NB**: Features follow a **normal distribution**\n",
        "   * **Multinomial NB**: Features are **counts (e.g., word frequencies)**\n",
        "   * **Bernoulli NB**: Features are **binary (0 or 1)**\n"
      ],
      "metadata": {
        "id": "7LHoEftBmS6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the advantages and disadvantages of Naïve Bayes?\n",
        "->  ✅ **Advantages of Naïve Bayes (Simple):**\n",
        "\n",
        "1. **Very fast** and easy to use\n",
        "2. Works well with **text data**\n",
        "3. Needs **less training data**\n",
        "4. Handles **many features** well\n",
        "\n",
        "\n",
        " ❌ **Disadvantages of Naïve Bayes (Simple):**\n",
        "\n",
        "1. Assumes features are **independent** (not true in real life)\n",
        "2. **Not good** if features are related\n",
        "3. Gives **bad probability scores** sometimes\n",
        "4. **Zero problem**: If something never appeared in training, it gives **0 probability**\n",
        "\n"
      ],
      "metadata": {
        "id": "RvrJll_Qmf_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Why is Naïve Bayes a good choice for text classification?\n",
        "->\n",
        "1. **Fast and Efficient**\n",
        "   👉 Works quickly even with **thousands of words** (features).\n",
        "\n",
        "2. **Handles High-Dimensional Data**\n",
        "   👉 Text data has many features (every word is a feature), and Naïve Bayes handles that well.\n",
        "\n",
        "3. **Works Well with Word Frequencies**\n",
        "   👉 Multinomial Naïve Bayes uses **word counts**, which is perfect for text.\n",
        "\n",
        "4. **Needs Less Data**\n",
        "   👉 Performs well even with **small training sets**.\n",
        "\n",
        "5. **Good Accuracy for Simple Problems**\n",
        "   👉 Great for tasks like **spam detection**, **sentiment analysis**, etc.\n"
      ],
      "metadata": {
        "id": "l07BrRdfm3US"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Compare SVM and Naïve Bayes for classification tasks?\n",
        "->\n",
        "\n",
        "| Point             | **SVM**              | **Naïve Bayes**                  |\n",
        "| ----------------- | -------------------- | -------------------------------- |\n",
        "| **Speed**         | Slower               | Very fast                        |\n",
        "| **Best For**      | Complex data, images | Text data, spam, simple tasks    |\n",
        "| **Assumption**    | No strong assumption | Assumes features are independent |\n",
        "| **Accuracy**      | High (if tuned)      | Good, but not always best        |\n",
        "| **Probabilities** | Not reliable         | Gives class probabilities        |\n",
        "| **Training Data** | Needs more           | Works well with small data       |\n",
        "\n"
      ],
      "metadata": {
        "id": "oV2vSy-gnFu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "->\n",
        "### 🧠 The Problem:\n",
        "\n",
        "* If a word (feature) **never appears** in the training data for a class, its probability becomes **zero**.\n",
        "* This makes the whole **class probability = 0**, which is wrong.\n",
        "\n",
        "\n",
        "### ✅ The Solution: **Laplace Smoothing**\n",
        "\n",
        "* It **adds 1** to every word count (even unseen ones).\n",
        "* This ensures **no zero probabilities**.\n",
        "\n",
        "\n",
        "### 📌 Formula (with Laplace Smoothing):\n",
        "\n",
        "$$\n",
        "P(w \\mid c) = \\frac{\\text{count}(w, c) + 1}{\\text{total words in } c + V}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\text{count}(w, c)$ = count of word $w$ in class $c$\n",
        "* $V$ = total number of unique words (vocabulary size)\n"
      ],
      "metadata": {
        "id": "k0GwNpy_neRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical questions"
      ],
      "metadata": {
        "id": "Locg921KnvIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of the SVM Classifier on the Iris dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPx01R-unzTC",
        "outputId": "2636d751-a325-4a7a-bda1-23a67e2069e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVM Classifier on the Iris dataset: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "# compare their accuracies\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqaOO6jZsWQb",
        "outputId": "dbd10ec7-ad30-413b-b0a1-54c5469f2916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 1.0000\n",
            "Accuracy of SVM with RBF Kernel: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "# Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def train_and_evaluate_svr():\n",
        "  \"\"\"\n",
        "  Trains an SVR model on the California housing dataset and evaluates it using MSE.\n",
        "  \"\"\"\n",
        "\n",
        "  housing = fetch_california_housing()\n",
        "  X = housing.data\n",
        "  y = housing.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  svr_model = SVR(kernel='linear')\n",
        "  svr_model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = svr_model.predict(X_test)\n",
        "\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "  print(f\"Mean Squared Error (MSE) of the SVR model: {mse:.4f}\")\n",
        "\n",
        "train_and_evaluate_svr()\n"
      ],
      "metadata": {
        "id": "mQlIfCQltCZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "# boundary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "def plot_decision_boundary(X, y, model, title):\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(X_test, y_test, svm_poly, \"SVM with Polynomial Kernel Decision Boundary\")\n",
        "\n",
        "accuracy = svm_poly.score(X_test, y_test)\n",
        "print(f\"Accuracy of the Polynomial SVM Classifier: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BhhJniDktmKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "# evaluate accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "def train_and_evaluate_gaussian_naive_bayes():\n",
        "  \"\"\"\n",
        "  Trains a Gaussian Naive Bayes classifier on the Breast Cancer dataset and evaluates accuracy.\n",
        "  \"\"\"\n",
        "  breast_cancer = load_breast_cancer()\n",
        "  X = breast_cancer.data\n",
        "  y = breast_cancer.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  gnb = GaussianNB()\n",
        "  gnb.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = gnb.predict(X_test)\n",
        "\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  print(f\"Accuracy of the Gaussian Naive Bayes classifier on the Breast Cancer dataset: {accuracy:.2f}\")\n",
        "\n",
        "train_and_evaluate_gaussian_naive_bayes()\n"
      ],
      "metadata": {
        "id": "PQ5V7OZUwOih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "# Newsgroups dataset\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_train, y_train = newsgroups_train.data, newsgroups_train.target\n",
        "X_test, y_test = newsgroups_test.data, newsgroups_test.target\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "=\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = mnb.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Multinomial Naive Bayes classifier: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups_test.target_names))\n"
      ],
      "metadata": {
        "id": "o1_olmyVw1Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "# boundaries visually\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def plot_decision_boundary_with_C(X, y, model, title):\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_clusters_per_class=1, random_state=42, flip_y=0.1) # Introduce some noise\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "c_values = [0.01, 1, 100]\n",
        "\n",
        "for c in c_values:\n",
        "\n",
        "    svm_classifier = SVC(kernel='linear', C=c)\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    plot_decision_boundary_with_C(X_train, y_train, svm_classifier, f\"SVM (C={c}) Decision Boundary on Training Data\")\n",
        "\n",
        "    accuracy = svm_classifier.score(X_test, y_test)\n",
        "    print(f\"Accuracy with C={c}: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tJrT0qbJxKdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "# binary features\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "X_binary, y_binary = make_classification(n_samples=100, n_features=5,\n",
        "                                         n_informative=3, n_redundant=0, n_clusters_per_class=1,\n",
        "                                         flip_y=0.05, random_state=42)\n",
        "\n",
        "median_values = np.median(X_binary, axis=0)\n",
        "X_binary_thresholded = (X_binary > median_values).astype(int)\n",
        "\n",
        "\n",
        "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(\n",
        "    X_binary_thresholded, y_binary, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train_binary, y_train_binary)\n",
        "\n",
        "y_pred_binary = bnb.predict(X_test_binary)\n",
        "\n",
        "accuracy_binary = accuracy_score(y_test_binary, y_pred_binary)\n",
        "\n",
        "print(f\"Accuracy of the Bernoulli Naive Bayes classifier: {accuracy_binary:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_binary, y_pred_binary))\n"
      ],
      "metadata": {
        "id": "kRncTonYxdUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "# unscaled data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def train_and_compare_scaled_unscaled_svm():\n",
        "  \"\"\"\n",
        "  Trains SVM models with and without feature scaling and compares their accuracy.\n",
        "  \"\"\"\n",
        "  iris = load_iris()\n",
        "  X = iris.data\n",
        "  y = iris.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  svm_unscaled = SVC(kernel='linear')\n",
        "  svm_unscaled.fit(X_train, y_train)\n",
        "  y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "  accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "  print(f\"Accuracy of SVM on unscaled data: {accuracy_unscaled:.4f}\")\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "  svm_scaled = SVC(kernel='linear')\n",
        "  svm_scaled.fit(X_train_scaled, y_train)\n",
        "  y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "  accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "  print(f\"Accuracy of SVM on scaled data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "train_and_compare_scaled_unscaled_svm()\n"
      ],
      "metadata": {
        "id": "MtxDkd2Lx6la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data\n",
        "\n",
        "# Re-using necessary imports from the preceding code\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def train_and_compare_laplace_smoothing():\n",
        "  \"\"\"\n",
        "  Trains Gaussian Naive Bayes models with and without Laplace smoothing and compares predictions.\n",
        "  Note: Gaussian Naive Bayes doesn't have an explicit Laplace smoothing parameter.\n",
        "  Laplace smoothing (alpha) is primarily for MultinomialNB and BernoulliNB,\n",
        "  used to handle zero probabilities for categorical or binary features.\n",
        "  For continuous features in GaussianNB, the issue of zero probability for unseen\n",
        "  values is handled by the probability density function, not by adding counts.\n",
        "\n",
        "  However, we can simulate the concept for demonstration purposes by comparing\n",
        "  predictions on a dataset where feature values might be extremely low or high,\n",
        "  or by illustrating the theoretical issue if we were using a different NB type.\n",
        "\n",
        "  A direct comparison of \"before and after Laplace smoothing\" on a GaussianNB\n",
        "  doesn't align with the model's nature. This function will instead demonstrate GNB\n",
        "  and briefly discuss the concept of smoothing as it applies to *other* NB types.\n",
        "  \"\"\"\n",
        "  print(\"Demonstrating Gaussian Naive Bayes (Laplace smoothing not directly applicable).\")\n",
        "  print(\"Laplace smoothing is primarily for Multinomial and Bernoulli NB to handle zero probabilities.\")\n",
        "\n",
        "  breast_cancer = load_breast_cancer()\n",
        "  X = breast_cancer.data\n",
        "  y = breast_cancer.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  gnb = GaussianNB()\n",
        "  gnb.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = gnb.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"\\nAccuracy of Gaussian Naive Bayes on Breast Cancer dataset: {accuracy:.4f}\")\n",
        "\n",
        "  print(\"\\nTheoretical consideration for Laplace smoothing:\")\n",
        "  print(\"If this were a Multinomial or Bernoulli NB and we encountered a feature value\")\n",
        "  print(\"in the test set that never appeared in the training set for a specific class,\")\n",
        "  print(\"the probability for that feature given the class would be calculated as zero.\")\n",
        "  print(\"This would make the entire class probability zero.\")\n",
        "  print(\"Laplace smoothing (adding a small constant like 1 to counts) prevents this.\")\n",
        "  print(\"Example (Multinomial NB): P(word|class) = (count(word, class) + alpha) / (total words in class + alpha * vocab_size)\")\n",
        "  print(\"For Gaussian NB, the PDF inherently handles values not seen during training.\")\n",
        "\n",
        "\n",
        "train_and_compare_laplace_smoothing()"
      ],
      "metadata": {
        "id": "mQPhBkX1yGRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "# gamma, kernel)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the best SVM model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "l9Ay8gazyTl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "# check it improve accuracy\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=0, n_clusters_per_class=1, weights=[0.9, 0.1],\n",
        "                           flip_y=0.05, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(f\"Training set class distribution: {dict(zip(unique, counts))}\")\n",
        "\n",
        "svm_no_weight = SVC(kernel='linear', random_state=42)\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "print(f\"\\nAccuracy of SVM without class weighting: {accuracy_no_weight:.4f}\")\n",
        "print(\"Classification Report (No Weighting):\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "svm_with_weight = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)\n",
        "print(f\"\\nAccuracy of SVM with class weighting: {accuracy_with_weight:.4f}\")\n",
        "print(\"Classification Report (With Weighting):\")\n",
        "print(classification_report(y_test, y_pred_with_weight))\n"
      ],
      "metadata": {
        "id": "7tblIyWozQm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 30. Write a Python program to train a Naïve Bayes classifier for spam detection using email data\n",
        "\n",
        "newsgroups_train_subset = fetch_20newsgroups(subset='train', categories=['soc.religion.christian', 'talk.politics.guns'],\n",
        "                                            remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test_subset = fetch_20newsgroups(subset='test', categories=['soc.religion.christian', 'talk.politics.guns'],\n",
        "                                           remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_train_subset, y_train_subset = newsgroups_train_subset.data, newsgroups_train_subset.target\n",
        "X_test_subset, y_test_subset = newsgroups_test_subset.data, newsgroups_test_subset.target\n",
        "\n",
        "\n",
        "target_names_map = {0: 'ham', 1: 'spam'}\n",
        "y_train_mapped = [target_names_map[label] for label in y_train_subset]\n",
        "y_test_mapped = [target_names_map[label] for label in y_test_subset]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "X_train_tfidf_subset = tfidf_vectorizer.fit_transform(X_train_subset)\n",
        "X_test_tfidf_subset = tfidf_vectorizer.transform(X_test_subset)\n",
        "\n",
        "mnb_spam_detector = MultinomialNB(alpha=1.0)\n",
        "mnb_spam_detector.fit(X_train_tfidf_subset, y_train_subset)\n",
        "\n",
        "y_pred_subset = mnb_spam_detector.predict(X_test_tfidf_subset)\n",
        "\n",
        "y_pred_mapped = [target_names_map[label] for label in y_pred_subset]\n",
        "\n",
        "accuracy_subset = accuracy_score(y_test_subset, y_pred_subset)\n",
        "print(f\"Accuracy of the Naive Bayes Spam Detector: {accuracy_subset:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_mapped, y_pred_mapped, target_names=['ham', 'spam']))\n",
        "\n",
        "new_emails = [\n",
        "    \"Free money now!!! Click this link to claim your prize.\",\n",
        "    \"Hello, hope you are doing well. Just checking in.\",\n",
        "    \"Win a million dollars! Limited time offer, sign up today.\",\n",
        "    \"Meeting scheduled for tomorrow at 10 AM.\"\n",
        "]\n",
        "\n",
        "new_emails_tfidf = tfidf_vectorizer.transform(new_emails)\n",
        "\n",
        "predictions = mnb_spam_detector.predict(new_emails_tfidf)\n",
        "predicted_labels = [target_names_map[label] for label in predictions]\n",
        "\n",
        "print(\"\\nPredictions on new emails:\")\n",
        "for email, label in zip(new_emails, predicted_labels):\n",
        "    print(f\"Email: '{email[:50]}...' -> Predicted: {label}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IOzDhLFNzfoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "# compare their accuracy\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"Accuracy of SVM Classifier: {accuracy_svm:.4f}\")\n",
        "\n",
        "gnb_classifier = GaussianNB()\n",
        "gnb_classifier.fit(X_train, y_train)\n",
        "y_pred_gnb = gnb_classifier.predict(X_test)\n",
        "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
        "print(f\"Accuracy of Gaussian Naive Bayes Classifier: {accuracy_gnb:.4f}\")\n",
        "\n",
        "print(\"\\nComparison of Accuracies:\")\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
        "print(f\"Gaussian Naive Bayes Accuracy: {accuracy_gnb:.4f}\")\n"
      ],
      "metadata": {
        "id": "uejXlShM0Vi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "# results\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "\n",
        "def train_and_compare_feature_selection_nb():\n",
        "  \"\"\"\n",
        "  Trains a Naive Bayes classifier with and without feature selection and compares accuracy.\n",
        "  Uses the Breast Cancer dataset for demonstration.\n",
        "  \"\"\"\n",
        "  breast_cancer = load_breast_cancer()\n",
        "  X = breast_cancer.data\n",
        "  y = breast_cancer.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  gnb_no_fs = GaussianNB()\n",
        "  gnb_no_fs.fit(X_train, y_train)\n",
        "  y_pred_no_fs = gnb_no_fs.predict(X_test)\n",
        "  accuracy_no_fs = accuracy_score(y_test, y_pred_no_fs)\n",
        "  print(f\"Accuracy of Naive Bayes without Feature Selection: {accuracy_no_fs:.4f}\")\n",
        "\n",
        "  k_features = 10\n",
        "  selector = SelectKBest(score_func=f_classif, k=k_features)\n",
        "  X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "  X_test_selected = selector.transform(X_test)\n",
        "\n",
        "  gnb_with_fs = GaussianNB()\n",
        "  gnb_with_fs.fit(X_train_selected, y_train)\n",
        "  y_pred_with_fs = gnb_with_fs.predict(X_test_selected)\n",
        "  accuracy_with_fs = accuracy_score(y_test, y_pred_with_fs)\n",
        "  print(f\"Accuracy of Naive Bayes with Feature Selection (Top {k_features} features): {accuracy_with_fs:.4f}\")\n",
        "\n",
        "  selected_features_indices = selector.get_support(indices=True)\n",
        "  print(f\"Indices of selected features: {selected_features_indices}\")\n",
        "  print(f\"Names of selected features: {breast_cancer.feature_names[selected_features_indices]}\")\n",
        "\n",
        "\n",
        "  print(\"\\nComparison of Accuracies:\")\n",
        "  print(f\"Naive Bayes Accuracy (No Feature Selection): {accuracy_no_fs:.4f}\")\n",
        "  print(f\"Naive Bayes Accuracy (With Feature Selection): {accuracy_with_fs:.4f}\")\n",
        "\n",
        "train_and_compare_feature_selection_nb()\n"
      ],
      "metadata": {
        "id": "wpyNVXZm0kQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "# strategies on the Wine dataset and compare their accuracy\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_ovr = SVC(decision_function_shape='ovr', kernel='linear', random_state=42)\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Accuracy of SVM with One-vs-Rest (OvR) strategy: {accuracy_ovr:.4f}\")\n",
        "\n",
        "svm_ovo = SVC(decision_function_shape='ovo', kernel='linear', random_state=42) # 'ovo' is the default\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "print(f\"Accuracy of SVM with One-vs-One (OvO) strategy: {accuracy_ovo:.4f}\")\n",
        "\n",
        "print(\"\\nComparison of Accuracies:\")\n",
        "print(f\"SVM (OvR) Accuracy: {accuracy_ovr:.4f}\")\n",
        "print(f\"SVM (OvO) Accuracy: {accuracy_ovo:.4f}\")\n"
      ],
      "metadata": {
        "id": "zNY4QLPD087r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "# Cancer dataset and compare their accuracy\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with Polynomial Kernel: {accuracy_poly:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "print(\"\\nComparison of Accuracies:\")\n",
        "print(f\"Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Polynomial Kernel: {accuracy_poly:.4f}\")\n",
        "print(f\"RBF Kernel: {accuracy_rbf:.4f}\")"
      ],
      "metadata": {
        "id": "HmaUTlcO1Hha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "# average accuracy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def train_svm_with_stratified_kfold():\n",
        "  \"\"\"\n",
        "  Trains an SVM classifier using Stratified K-Fold cross-validation and computes the average accuracy.\n",
        "  Uses the Iris dataset for demonstration.\n",
        "  \"\"\"\n",
        "  iris = load_iris()\n",
        "  X = iris.data\n",
        "  y = iris.target\n",
        "\n",
        "  n_splits = 5\n",
        "\n",
        "  skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "  accuracies = []\n",
        "\n",
        "  for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"Fold {fold + 1}/{n_splits}\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = svm_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"  Accuracy for this fold: {accuracy:.4f}\")\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "  average_accuracy = np.mean(accuracies)\n",
        "  print(f\"\\nAverage accuracy across {n_splits} folds: {average_accuracy:.4f}\")\n",
        "\n",
        "train_svm_with_stratified_kfold()\n"
      ],
      "metadata": {
        "id": "ERGBACYZ1gEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "# performance\n",
        "\n",
        "import numpy as np\n",
        "def train_naive_bayes_with_different_priors():\n",
        "  \"\"\"\n",
        "  Trains a Gaussian Naive Bayes classifier with different prior probabilities\n",
        "  and compares performance on the Breast Cancer dataset.\n",
        "  \"\"\"\n",
        "  breast_cancer = load_breast_cancer()\n",
        "  X = breast_cancer.data\n",
        "  y = breast_cancer.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "  unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "  actual_priors = class_counts / len(y_train)\n",
        "  print(f\"Actual class priors in training data: {dict(zip(unique_classes, actual_priors))}\")\n",
        "\n",
        "\n",
        "  prior_settings = {\n",
        "      'Default (Learned)': None,\n",
        "      'Uniform': [0.5, 0.5],\n",
        "      'Skewed (Favor Class 0)': [0.8, 0.2]\n",
        "  }\n",
        "\n",
        "  results = {}\n",
        "\n",
        "  for name, priors in prior_settings.items():\n",
        "    print(f\"\\nTraining with priors: {name}\")\n",
        "    if priors is not None and len(priors) != len(unique_classes):\n",
        "        print(f\"  Warning: Number of custom priors ({len(priors)}) does not match number of classes ({len(unique_classes)}). Skipping.\")\n",
        "        continue\n",
        "\n",
        "    gnb = GaussianNB(priors=priors)\n",
        "    gnb.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = gnb.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'classification_report': report\n",
        "    }\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(\"  Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "  print(\"\\n--- Summary of Performance with Different Priors ---\")\n",
        "  for name, metrics in results.items():\n",
        "    print(f\"\\nPrior Setting: {name}\")\n",
        "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(\"  Precision (Class 0): {:.4f}\".format(metrics['classification_report']['0']['precision']))\n",
        "    print(\"  Recall (Class 0): {:.4f}\".format(metrics['classification_report']['0']['recall']))\n",
        "    print(\"  Precision (Class 1): {:.4f}\".format(metrics['classification_report']['1']['precision']))\n",
        "    print(\"  Recall (Class 1): {:.4f}\".format(metrics['classification_report']['1']['recall']))\n",
        "\n",
        "\n",
        "train_naive_bayes_with_different_priors()"
      ],
      "metadata": {
        "id": "lTNxd42v148a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "# compare accuracy\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def train_svm_with_rfe_and_compare():\n",
        "  \"\"\"\n",
        "  Trains an SVM Classifier with and without Recursive Feature Elimination (RFE)\n",
        "  and compares their accuracy on the Iris dataset.\n",
        "  \"\"\"\n",
        "  iris = load_iris()\n",
        "  X = iris.data\n",
        "  y = iris.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  svm_no_rfe = SVC(kernel='linear', random_state=42)\n",
        "  svm_no_rfe.fit(X_train, y_train)\n",
        "  y_pred_no_rfe = svm_no_rfe.predict(X_test)\n",
        "  accuracy_no_rfe = accuracy_score(y_test, y_pred_no_rfe)\n",
        "  print(f\"Accuracy of SVM without RFE: {accuracy_no_rfe:.4f}\")\n",
        "\n",
        "  n_features_to_select = 2\n",
        "  estimator = SVC(kernel='linear')\n",
        "\n",
        "  rfe = RFE(estimator=estimator, n_features_to_select=n_features_to_select)\n",
        "\n",
        "  rfe.fit(X_train, y_train)\n",
        "\n",
        "  X_train_rfe = rfe.transform(X_train)\n",
        "  X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "  svm_with_rfe = SVC(kernel='linear', random_state=42)\n",
        "  svm_with_rfe.fit(X_train_rfe, y_train)\n",
        "  y_pred_with_rfe = svm_with_rfe.predict(X_test_rfe)\n",
        "  accuracy_with_rfe = accuracy_score(y_test, y_pred_with_rfe)\n",
        "  print(f\"Accuracy of SVM with RFE (Selected {n_features_to_select} features): {accuracy_with_rfe:.4f}\")\n",
        "\n",
        "  selected_features_mask = rfe.support_\n",
        "  selected_feature_indices = np.where(selected_features_mask)[0]\n",
        "  print(f\"Indices of selected features by RFE: {selected_feature_indices}\")\n",
        "  print(f\"Names of selected features: {iris.feature_names[selected_feature_indices]}\")\n",
        "\n",
        "  print(\"\\n--- Comparison of Accuracies ---\")\n",
        "  print(f\"Accuracy without RFE: {accuracy_no_rfe:.4f}\")\n",
        "  print(f\"Accuracy with RFE: {accuracy_with_rfe:.4f}\")\n",
        "\n",
        "train_svm_with_rfe_and_compare()\n"
      ],
      "metadata": {
        "id": "LNA8T38z2487"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "# F1-Score instead of accuracy\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"SVM Classifier Performance:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "id": "g5e_CLyD3X9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "# (Cross-Entropy Loss)\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def train_and_evaluate_naive_bayes_log_loss():\n",
        "  \"\"\"\n",
        "  Trains a Gaussian Naive Bayes classifier and evaluates its performance using Log Loss.\n",
        "  Uses the Breast Cancer dataset for demonstration.\n",
        "  \"\"\"\n",
        "  breast_cancer = load_breast_cancer()\n",
        "  X = breast_cancer.data\n",
        "  y = breast_cancer.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  gnb = GaussianNB()\n",
        "  gnb.fit(X_train, y_train)\n",
        "\n",
        "  y_proba = gnb.predict_proba(X_test)\n",
        "\n",
        "  logloss = log_loss(y_test, y_proba)\n",
        "\n",
        "  print(f\"Log Loss (Cross-Entropy) of the Gaussian Naive Bayes classifier: {logloss:.4f}\")\n",
        "\n",
        "  y_pred = gnb.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Accuracy of the Gaussian Naive Bayes classifier: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "train_and_evaluate_naive_bayes_log_loss()\n"
      ],
      "metadata": {
        "id": "cVYPAWzp3x0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for SVM Classifier on Iris Dataset')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the SVM Classifier: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "yFW1D9Jz4NMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "# Error (MAE) instead of MSE\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def train_and_evaluate_svr_mae():\n",
        "  \"\"\"\n",
        "  Trains an SVR model on the California housing dataset and evaluates it using MAE.\n",
        "  \"\"\"\n",
        "\n",
        "  housing = fetch_california_housing()\n",
        "  X = housing.data\n",
        "  y = housing.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  svr_model = SVR(kernel='linear')\n",
        "  svr_model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = svr_model.predict(X_test)\n",
        "\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "  print(f\"Mean Absolute Error (MAE) of the SVR model: {mae:.4f}\")\n",
        "\n",
        "train_and_evaluate_svr_mae()\n"
      ],
      "metadata": {
        "id": "lvaW3IGF4eoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 45.  Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "# score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import confusion_matrix, mean_absolute_error, log_loss, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "\n",
        "def train_and_evaluate_naive_bayes_roc_auc():\n",
        "  \"\"\"\n",
        "  Trains a Naive Bayes classifier and evaluates its performance using ROC-AUC score.\n",
        "  Uses the Breast Cancer dataset for demonstration.\n",
        "  \"\"\"\n",
        "  breast_cancer = load_breast_cancer()\n",
        "  X = breast_cancer.data\n",
        "  y = breast_cancer.target\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  gnb = GaussianNB()\n",
        "  gnb.fit(X_train, y_train)\n",
        "\n",
        "  if len(np.unique(y_test)) == 2:\n",
        "    y_proba = gnb.predict_proba(X_test)[:, 1]\n",
        "    auc_score = roc_auc_score(y_test, y_proba)\n",
        "    print(f\"ROC-AUC Score for Naive Bayes Classifier: {auc_score:.4f}\")\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "  else:\n",
        "    print(\"Dataset has multiple classes. Calculating macro-average ROC-AUC.\")\n",
        "    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
        "    y_proba = gnb.predict_proba(X_test)\n",
        "\n",
        "    auc_scores = []\n",
        "    for i in range(y_test_bin.shape[1]):\n",
        "        try:\n",
        "            score = roc_auc_score(y_test_bin[:, i], y_proba[:, i])\n",
        "            auc_scores.append(score)\n",
        "        except ValueError as e:\n",
        "            print(f\"Could not compute ROC-AUC for class {i}: {e}\")\n",
        "\n",
        "\n",
        "    if auc_scores:\n",
        "        macro_avg_auc = np.mean(auc_scores)\n",
        "        print(f\"Macro-average ROC-AUC Score for Naive Bayes Classifier: {macro_avg_auc:.4f}\")\n",
        "    else:\n",
        "        print(\"Could not compute ROC-AUC for any class.\")\n",
        "\n",
        "\n",
        "train_and_evaluate_naive_bayes_roc_auc()\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "kgqrdLPS4mhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = make_classification(n_samples=200, n_features=10, n_informative=5,\n",
        "                           n_redundant=0, n_clusters_per_class=1, flip_y=0.1, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_classifier = SVC(kernel='linear', random_state=42, probability=True)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_scores = svm_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "auprc = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (area = {auprc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for SVM Classifier')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc:.4f}\")\n"
      ],
      "metadata": {
        "id": "Pb3uGSBq41NG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}